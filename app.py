import streamlit as st
import os
import cv2
import numpy as np
from moviepy.editor import VideoFileClip, AudioFileClip, vfx
import google.generativeai as genai
import edge_tts
import asyncio
import tempfile
import time

# --- CONFIGURATION ---
GEMINI_API_KEY = "AIzaSyBDfSFCV4kF56dAZ8Zx0m0xaR8a40v8pG4"

# âš ï¸ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€†á€¯á€¶á€¸á€¡á€•á€­á€¯á€„á€ºá€¸- v1beta Error á€€á€­á€¯ á€€á€»á€±á€¬á€ºá€›á€”á€º API Version á€€á€­á€¯ v1 á€¡á€–á€¼á€…á€º Force á€œá€¯á€•á€ºá€á€¼á€„á€ºá€¸
os.environ["GOOGLE_GENERATIVE_AI_API_VERSION"] = "v1"
genai.configure(api_key=GEMINI_API_KEY)

st.set_page_config(page_title="Auto Burmese Movie Recap AI", layout="wide")

# --- FUNCTIONS ---

def adjust_video_sync(video_path, audio_path, output_path):
    video_clip = VideoFileClip(video_path).without_audio()
    audio_clip = AudioFileClip(audio_path)
    speed_factor = video_clip.duration / audio_clip.duration
    final_video = video_clip.fx(vfx.speedx, speed_factor).set_audio(audio_clip)
    final_video.write_videofile(output_path, codec="libx264", audio_codec="aac", fps=24)
    return output_path

def apply_blur_to_video(video_path, output_path):
    cap = cv2.VideoCapture(video_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = cap.get(cv2.CAP_PROP_FPS) or 24
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        logo = frame[10:110, width-210:width-10]
        if logo.size > 0: frame[10:110, width-210:width-10] = cv2.GaussianBlur(logo, (51, 51), 0)
        sub = frame[height-140:height-10, 50:width-50]
        if sub.size > 0: frame[height-140:height-10, 50:width-50] = cv2.GaussianBlur(sub, (51, 51), 0)
        out.write(frame)
    cap.release(); out.release()
    return output_path

async def generate_voice(text, output_path):
    communicate = edge_tts.Communicate(text, "my-MM-ThihaNeural")
    await asyncio.wait_for(communicate.save(output_path), timeout=60)

def analyze_and_recap(video_file_path):
    # Model á€”á€¬á€™á€Šá€ºá€€á€­á€¯ models/ á€™á€•á€«á€˜á€² á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€á€±á€«á€ºá€€á€¼á€Šá€·á€ºá€á€¼á€„á€ºá€¸ (Version v1 á€¡á€á€½á€€á€º)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    st.write("ğŸ“¤ á€—á€®á€’á€®á€šá€­á€¯á€–á€­á€¯á€„á€ºá€€á€­á€¯ AI á€†á€® á€á€„á€ºá€•á€­á€¯á€·á€”á€±á€á€Šá€º...")
    video_file = genai.upload_file(path=video_file_path)
    
    while video_file.state.name == "PROCESSING":
        time.sleep(2)
        video_file = genai.get_file(video_file.name)

    prompt = (
        "Watch this video and listen to the audio carefully. "
        "Summarize the story and translate it into a dramatic Burmese movie recap script. "
        "Start with 'á€‡á€¬á€á€ºá€œá€™á€ºá€¸á€…á€…á€á€»á€„á€ºá€¸á€™á€¾á€¬...'. Output Burmese text only."
    )
    
    response = model.generate_content([video_file, prompt])
    return response.text

# --- UI ---
st.title("ğŸ¬ Burmese Movie Recap AI")
uploaded_file = st.file_uploader("á€—á€®á€’á€®á€šá€­á€¯á€–á€­á€¯á€„á€ºá€á€„á€ºá€•á€«", type=['mp4', 'webm', 'mov', 'avi'])

if uploaded_file:
    suffix = os.path.splitext(uploaded_file.name)[1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tfile:
        tfile.write(uploaded_file.read())
        temp_path = tfile.name
    
    if st.button("á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º Recap á€•á€¼á€¯á€œá€¯á€•á€ºá€•á€«"):
        with st.status("AI á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€”á€±á€á€Šá€º...", expanded=True) as status:
            try:
                # 1. AI Analysis
                st.write("ğŸ•µï¸ AI á€€ á€—á€®á€’á€®á€šá€­á€¯á€€á€­á€¯ á€”á€¬á€¸á€‘á€±á€¬á€„á€ºá€”á€±á€á€Šá€º (á€á€±á€á€¹á€á€…á€±á€¬á€„á€·á€ºá€•á€«)...")
                script = analyze_and_recap(temp_path)
                st.success("á€‡á€¬á€á€ºá€Šá€½á€¾á€”á€ºá€¸ á€›á€›á€¾á€­á€•á€«á€•á€¼á€®!")
                
                # 2. Voice
                st.write("ğŸ™ï¸ á€—á€™á€¬á€¡á€á€¶á€á€½á€„á€ºá€¸á€”á€±á€á€Šá€º...")
                asyncio.run(generate_voice(script, "voice.mp3"))
                
                # 3. Processing
                st.write("ğŸŒ«ï¸ á€—á€®á€’á€®á€šá€­á€¯á€€á€­á€¯ á€•á€¼á€¯á€•á€¼á€„á€ºá€”á€±á€á€Šá€º...")
                blurred = apply_blur_to_video(temp_path, "blurred.mp4")
                final = adjust_video_sync(blurred, "voice.mp3", "final.mp4")
                
                status.update(label="âœ… á€¡á€¬á€¸á€œá€¯á€¶á€¸ á€•á€¼á€®á€¸á€•á€«á€•á€¼á€®!", state="complete")
                st.video(final)
                with open(final, "rb") as f:
                    st.download_button("ğŸ“¥ Download Video", f, "recap.mp4")
            except Exception as e:
                st.error(f"Error: {str(e)}")
            finally:
                if os.path.exists(temp_path): os.remove(temp_path)
